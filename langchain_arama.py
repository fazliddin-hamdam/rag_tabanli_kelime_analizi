# langchain_arama.py

import os
import chromadb
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from sentence_transformers import SentenceTransformer

class TurkishSemanticSearch:
    """Langchain ve ChromaDB kullanarak TÃ¼rkÃ§e semantik arama sistemi"""
    
    def __init__(self, db_path="./db"):
        self.db_path = db_path
        self.embedding_model_name = "dbmdz/bert-base-turkish-cased"
        self.embeddings = None
        self.word_vectorstore = None
        self.sentence_vectorstore = None
        self._setup_embeddings()
    
    def _setup_embeddings(self):
        """Embedding modelini hazÄ±rla"""
        try:
            # HuggingFace embeddings with Turkish BERT model
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_name,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            print("âœ… Langchain embedding modeli hazÄ±rlandÄ±")
        except Exception as e:
            print(f"âŒ Embedding modeli hatasÄ±: {e}")
            raise e
    
    def setup_word_vectorstore(self):
        """Kelime vektÃ¶r deposunu kur"""
        try:
            # ChromaDB client
            client = chromadb.PersistentClient(path=self.db_path)
            
            # Langchain ile Chroma vectorstore
            self.word_vectorstore = Chroma(
                client=client,
                collection_name="kelime_vektorleri_langchain",
                embedding_function=self.embeddings,
                persist_directory=self.db_path
            )
            print("âœ… Kelime vektÃ¶r deposu hazÄ±r")
            return True
            
        except Exception as e:
            print(f"âŒ Kelime vektÃ¶r deposu hatasÄ±: {e}")
            return False
    
    def setup_sentence_vectorstore(self):
        """CÃ¼mle vektÃ¶r deposunu kur"""
        try:
            # ChromaDB client
            client = chromadb.PersistentClient(path=self.db_path)
            
            # Langchain ile Chroma vectorstore
            self.sentence_vectorstore = Chroma(
                client=client,
                collection_name="metin_vektorleri_langchain",
                embedding_function=self.embeddings,
                persist_directory=self.db_path
            )
            print("âœ… CÃ¼mle vektÃ¶r deposu hazÄ±r")
            return True
            
        except Exception as e:
            print(f"âŒ CÃ¼mle vektÃ¶r deposu hatasÄ±: {e}")
            return False
    
    def add_words_to_vectorstore(self, words_file="kelimeler.txt"):
        """Kelimeleri vektÃ¶r deposuna ekle"""
        if not self.word_vectorstore:
            self.setup_word_vectorstore()
        
        try:
            # Kelimeleri yÃ¼kle
            with open(words_file, "r", encoding="utf-8") as f:
                words = [line.strip() for line in f if line.strip()]
            
            # Document objelerini oluÅŸtur
            documents = [
                Document(
                    page_content=word,
                    metadata={"type": "word", "index": i}
                ) for i, word in enumerate(words)
            ]
            
            # VektÃ¶r deposuna ekle
            self.word_vectorstore.add_documents(documents)
            print(f"âœ… {len(words)} kelime vektÃ¶r deposuna eklendi")
            return True
            
        except Exception as e:
            print(f"âŒ Kelime ekleme hatasÄ±: {e}")
            return False
    
    def add_sentences_to_vectorstore(self, sentences_file="metinler.txt"):
        """CÃ¼mleleri vektÃ¶r deposuna ekle"""
        if not self.sentence_vectorstore:
            self.setup_sentence_vectorstore()
        
        try:
            # CÃ¼mleleri yÃ¼kle
            with open(sentences_file, "r", encoding="utf-8") as f:
                sentences = [line.strip() for line in f if line.strip()]
            
            # Document objelerini oluÅŸtur
            documents = [
                Document(
                    page_content=sentence,
                    metadata={"type": "sentence", "index": i}
                ) for i, sentence in enumerate(sentences)
            ]
            
            # Batch olarak ekle
            batch_size = 100
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i+batch_size]
                self.sentence_vectorstore.add_documents(batch)
                print(f"  ğŸ“¦ {min(i+batch_size, len(documents))}/{len(documents)} cÃ¼mle iÅŸlendi...")
            
            print(f"âœ… {len(sentences)} cÃ¼mle vektÃ¶r deposuna eklendi")
            return True
            
        except Exception as e:
            print(f"âŒ CÃ¼mle ekleme hatasÄ±: {e}")
            return False
    
    def search_words(self, query, k=5):
        """Kelimelerde arama yap"""
        if not self.word_vectorstore:
            return []
        
        try:
            # Similarity search
            results = self.word_vectorstore.similarity_search_with_score(query, k=k)
            
            formatted_results = []
            for i, (doc, score) in enumerate(results):
                # Score'u similarity'ye Ã§evir (1 - score, Ã§Ã¼nkÃ¼ dÃ¼ÅŸÃ¼k score = yÃ¼ksek benzerlik)
                similarity = max(0, 1 - score)
                
                formatted_results.append({
                    'rank': i + 1,
                    'word': doc.page_content,
                    'similarity': similarity,
                    'similarity_percent': round(similarity * 100, 1),
                    'index': doc.metadata.get('index', i),
                    'score': score
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ Kelime arama hatasÄ±: {e}")
            return []
    
    def search_sentences(self, query, k=5):
        """CÃ¼mlelerde arama yap"""
        if not self.sentence_vectorstore:
            return []
        
        try:
            # Similarity search
            results = self.sentence_vectorstore.similarity_search_with_score(query, k=k)
            
            formatted_results = []
            for i, (doc, score) in enumerate(results):
                # Score'u similarity'ye Ã§evir (1 - score, Ã§Ã¼nkÃ¼ dÃ¼ÅŸÃ¼k score = yÃ¼ksek benzerlik)
                similarity = max(0, 1 - score)
                
                formatted_results.append({
                    'rank': i + 1,
                    'sentence': doc.page_content,
                    'similarity': similarity,
                    'similarity_percent': round(similarity * 100, 1),
                    'index': doc.metadata.get('index', i),
                    'score': score
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ CÃ¼mle arama hatasÄ±: {e}")
            return []
    
    def get_stats(self):
        """Ä°statistikleri getir"""
        stats = {
            'words_count': 0,
            'sentences_count': 0,
            'embedding_model': self.embedding_model_name,
            'db_path': self.db_path
        }
        
        try:
            if self.word_vectorstore:
                # Koleksiyon sayÄ±sÄ±nÄ± almaya Ã§alÄ±ÅŸ
                word_collection = self.word_vectorstore._collection
                stats['words_count'] = word_collection.count() if word_collection else 0
        except:
            pass
        
        try:
            if self.sentence_vectorstore:
                # Koleksiyon sayÄ±sÄ±nÄ± almaya Ã§alÄ±ÅŸ
                sentence_collection = self.sentence_vectorstore._collection
                stats['sentences_count'] = sentence_collection.count() if sentence_collection else 0
        except:
            pass
        
        return stats

# Test fonksiyonu
def test_langchain_search():
    """Langchain arama sistemini test et"""
    print("ğŸ”¬ Langchain Arama Sistemi Test Ediliyor...")
    
    # Arama sistemini baÅŸlat
    search_system = TurkishSemanticSearch()
    
    # VektÃ¶r depolarÄ±nÄ± kur
    search_system.setup_word_vectorstore()
    search_system.setup_sentence_vectorstore()
    
    # Ä°statistikleri gÃ¶ster
    stats = search_system.get_stats()
    print(f"ğŸ“Š Ä°statistikler: {stats}")
    
    # Test aramalarÄ±
    test_queries = ["kitap", "okul", "mutluluk"]
    
    for query in test_queries:
        print(f"\nğŸ” '{query}' aramasÄ±:")
        
        # Kelime aramasÄ±
        word_results = search_system.search_words(query, k=3)
        print(f"  ğŸ“ Kelimeler: {[r['word'] for r in word_results[:3]]}")
        
        # CÃ¼mle aramasÄ±
        sentence_results = search_system.search_sentences(query, k=3)
        print(f"  ğŸ“š CÃ¼mleler: {[r['sentence'][:50] + '...' for r in sentence_results[:3]]}")

if __name__ == "__main__":
    test_langchain_search() 