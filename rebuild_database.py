#!/usr/bin/env python3
"""
ğŸš€ Multi-Model ChromaDB Database Rebuild - Optimized Batch Processing
Ã‡oklu model destekli tÃ¼m veritabanÄ± koleksiyonlarÄ±nÄ± optimized batch processing ile yeniden oluÅŸturur.
"""

import os
import sys
import time
import shutil
import numpy as np
import chromadb
from pathlib import Path
from sentence_transformers import SentenceTransformer

# Desteklenecek modellerin tanÄ±mÄ±
SUPPORTED_MODELS = {
    "dbmdz_bert": "dbmdz/bert-base-turkish-cased",
    "turkcell_roberta": "TURKCELL/roberta-base-turkish-uncased",
    "multilingual_mpnet": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
}

def clear_database():
    """Mevcut veritabanÄ±nÄ± temizle"""
    db_dir = "db"
    if os.path.exists(db_dir):
        print(f"ğŸ—‘ï¸  Mevcut veritabanÄ± temizleniyor: {db_dir}")
        shutil.rmtree(db_dir)
    os.makedirs(db_dir, exist_ok=True)
    print("âœ… Temiz veritabanÄ± dizini oluÅŸturuldu")

def check_files():
    """Gerekli dosyalarÄ± kontrol et"""
    required_files = [
        "kelimeler.txt",
        "metinler.txt", 
        "iliskiler.txt"
    ]
    
    missing_files = []
    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)
    
    if missing_files:
        print(f"âŒ Eksik dosyalar: {', '.join(missing_files)}")
        return False
    
    print("âœ… TÃ¼m gerekli dosyalar mevcut")
    return True

def create_vectors_for_model(model_id, model_name, kelimeler, metinler):
    """Belirtilen model iÃ§in vektÃ¶rleri oluÅŸtur"""
    print(f"\nğŸ¤– Model yÃ¼kleniyor: {model_name}")
    try:
        model = SentenceTransformer(model_name)
        print(f"âœ… Model baÅŸarÄ±yla yÃ¼klendi: {model_id}")
        
        # Kelime vektÃ¶rlerini oluÅŸtur
        print(f"ğŸ”¤ Kelime vektÃ¶rleri oluÅŸturuluyor...")
        kelime_vektorleri = model.encode(kelimeler, show_progress_bar=True)
        
        # CÃ¼mle vektÃ¶rlerini oluÅŸtur
        print(f"ğŸ“š CÃ¼mle vektÃ¶rleri oluÅŸturuluyor...")
        metin_vektorleri = model.encode(metinler, show_progress_bar=True)
        
        return kelime_vektorleri, metin_vektorleri, True
        
    except Exception as e:
        print(f"âŒ Model {model_id} yÃ¼klenirken hata: {e}")
        return None, None, False

def rebuild_collections_for_model(model_id, kelimeler, metinler, kelime_vektorleri, metin_vektorleri):
    """Belirtilen model iÃ§in ChromaDB koleksiyonlarÄ±nÄ± oluÅŸtur"""
    print(f"\nğŸ’¾ {model_id} iÃ§in koleksiyonlar oluÅŸturuluyor...")
    
    # ChromaDB baÄŸlantÄ±sÄ±
    client = chromadb.PersistentClient(path="db")
    
    # Koleksiyon isimleri
    word_collection_name = f"kelime_vektorleri_{model_id}"
    sentence_collection_name = f"metin_vektorleri_{model_id}"
    
    try:
        # Kelime koleksiyonu
        print(f"ğŸ“– Kelime koleksiyonu oluÅŸturuluyor: {word_collection_name}")
        word_collection = client.get_or_create_collection(
            name=word_collection_name,
            metadata={"hnsw:space": "cosine", "model_id": model_id}
        )
        
        # Kelime vektÃ¶rlerini batch processing ile ekle
        batch_size = 1000
        for i in range(0, len(kelimeler), batch_size):
            end_idx = min(i + batch_size, len(kelimeler))
            
            batch_kelimeler = kelimeler[i:end_idx]
            batch_vektorler = kelime_vektorleri[i:end_idx]
            batch_ids = [str(j) for j in range(i, end_idx)]
            batch_embeddings = [vec.tolist() for vec in batch_vektorler]
            
            word_collection.add(
                embeddings=batch_embeddings,
                documents=batch_kelimeler,
                ids=batch_ids
            )
            
            print(f"  ğŸ“¦ {end_idx}/{len(kelimeler)} kelime iÅŸlendi...")
        
        print(f"âœ… Kelime koleksiyonu tamamlandÄ±: {word_collection.count()} kayÄ±t")
        
        # CÃ¼mle koleksiyonu
        print(f"ğŸ“ CÃ¼mle koleksiyonu oluÅŸturuluyor: {sentence_collection_name}")
        sentence_collection = client.get_or_create_collection(
            name=sentence_collection_name,
            metadata={"hnsw:space": "cosine", "model_id": model_id}
        )
        
        # CÃ¼mle vektÃ¶rlerini batch processing ile ekle
        batch_size = 500
        for i in range(0, len(metinler), batch_size):
            end_idx = min(i + batch_size, len(metinler))
            
            batch_metinler = metinler[i:end_idx]
            batch_vektorler = metin_vektorleri[i:end_idx]
            batch_ids = [str(j) for j in range(i, end_idx)]
            batch_embeddings = [vec.tolist() for vec in batch_vektorler]
            
            sentence_collection.add(
                embeddings=batch_embeddings,
                documents=batch_metinler,
                ids=batch_ids
            )
            
            print(f"  ğŸ“¦ {end_idx}/{len(metinler)} cÃ¼mle iÅŸlendi...")
        
        print(f"âœ… CÃ¼mle koleksiyonu tamamlandÄ±: {sentence_collection.count()} kayÄ±t")
        return True
        
    except Exception as e:
        print(f"âŒ {model_id} koleksiyonlarÄ± oluÅŸturulurken hata: {e}")
        return False

def verify_database():
    """VeritabanÄ±nÄ± doÄŸrula"""
    print("\nğŸ” VERÄ°TABANI DOÄRULAMA")
    print("=" * 50)
    
    try:
        client = chromadb.PersistentClient(path="db")
        
        # KoleksiyonlarÄ± kontrol et
        collections = client.list_collections()
        print(f"ğŸ“‹ Bulunan koleksiyonlar: {len(collections)}")
        
        model_stats = {}
        for collection_info in collections:
            collection = client.get_collection(collection_info.name)
            count = collection.count()
            print(f"  â€¢ {collection_info.name}: {count} kayÄ±t")
            
            # Model istatistiklerini topla
            for model_id in SUPPORTED_MODELS.keys():
                if model_id in collection_info.name:
                    if model_id not in model_stats:
                        model_stats[model_id] = {'words': 0, 'sentences': 0}
                    if 'kelime' in collection_info.name:
                        model_stats[model_id]['words'] = count
                    elif 'metin' in collection_info.name:
                        model_stats[model_id]['sentences'] = count
        
        # Model bazlÄ± istatistikler
        print(f"\nğŸ“Š MODEL Ä°STATÄ°STÄ°KLERÄ°:")
        for model_id, stats in model_stats.items():
            model_name = SUPPORTED_MODELS.get(model_id, "Unknown")
            print(f"  ğŸ¤– {model_name} ({model_id}):")
            print(f"     - Kelimeler: {stats['words']}")
            print(f"     - CÃ¼mleler: {stats['sentences']}")
        
        # Test aramasÄ± yap
        print(f"\nğŸ§ª TEST ARAMALARI:")
        for model_id in SUPPORTED_MODELS.keys():
            try:
                word_collection = client.get_collection(f"kelime_vektorleri_{model_id}")
                sentence_collection = client.get_collection(f"metin_vektorleri_{model_id}")
                
                # Test queries
                word_results = word_collection.query(query_texts=["test"], n_results=3)
                sentence_results = sentence_collection.query(query_texts=["test cÃ¼mlesi"], n_results=3)
                
                print(f"  âœ… {model_id}: Kelime={len(word_results['documents'][0])}, CÃ¼mle={len(sentence_results['documents'][0])}")
                
            except Exception as e:
                print(f"  âŒ {model_id}: Test baÅŸarÄ±sÄ±z - {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ VeritabanÄ± doÄŸrulama hatasÄ±: {e}")
        return False

def main():
    print("ğŸ¯ Multi-Model ChromaDB Database Rebuild")
    print("=" * 60)
    print(f"ğŸ¤– Desteklenen modeller: {len(SUPPORTED_MODELS)}")
    for model_id, model_name in SUPPORTED_MODELS.items():
        print(f"   â€¢ {model_id}: {model_name}")
    print("=" * 60)
    
    total_start_time = time.time()
    
    # DosyalarÄ± kontrol et
    if not check_files():
        print("âŒ Gerekli dosyalar eksik. Ä°ÅŸlem sonlandÄ±rÄ±lÄ±yor.")
        return False
    
    # VeritabanÄ±nÄ± temizle
    clear_database()
    
    # Verileri yÃ¼kle
    print("\nğŸ“– TEMEL VERÄ°LER YÃœKLENÄ°YOR")
    print("=" * 40)
    
    with open("kelimeler.txt", "r", encoding="utf-8") as f:
        kelimeler = [line.strip() for line in f if line.strip()]
    
    with open("metinler.txt", "r", encoding="utf-8") as f:
        metinler = [line.strip() for line in f if line.strip()]
    
    print(f"ğŸ“Š YÃ¼klenen veri:")
    print(f"   â€¢ Kelimeler: {len(kelimeler)}")
    print(f"   â€¢ CÃ¼mleler: {len(metinler)}")
    
    # Her model iÃ§in iÅŸlemleri yap
    successful_models = []
    failed_models = []
    
    for model_id, model_name in SUPPORTED_MODELS.items():
        print(f"\n{'='*60}")
        print(f"ğŸš€ {model_id.upper()} MODELÄ° Ä°ÅLENÄ°YOR")
        print(f"{'='*60}")
        
        model_start_time = time.time()
        
        # VektÃ¶rleri oluÅŸtur
        kelime_vektorleri, metin_vektorleri, success = create_vectors_for_model(
            model_id, model_name, kelimeler, metinler
        )
        
        if not success:
            failed_models.append(model_id)
            print(f"âŒ {model_id} modeli baÅŸarÄ±sÄ±z oldu, atlanÄ±yor...")
            continue
        
        # KoleksiyonlarÄ± oluÅŸtur
        if rebuild_collections_for_model(model_id, kelimeler, metinler, kelime_vektorleri, metin_vektorleri):
            model_end_time = time.time()
            model_duration = model_end_time - model_start_time
            successful_models.append(model_id)
            print(f"âœ… {model_id} modeli tamamlandÄ±: {model_duration:.2f} saniye")
        else:
            failed_models.append(model_id)
            print(f"âŒ {model_id} koleksiyonlarÄ± oluÅŸturulamadÄ±")
    
    # Final verification
    print(f"\n{'='*60}")
    if successful_models:
        verify_database()
    
    # Ã–zet rapor
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    
    print(f"\nğŸ Ä°ÅLEM TAMAMLANDI")
    print(f"{'='*40}")
    print(f"â±ï¸  Toplam sÃ¼re: {total_duration:.2f} saniye")
    print(f"âœ… BaÅŸarÄ±lÄ± modeller: {len(successful_models)} - {successful_models}")
    print(f"âŒ BaÅŸarÄ±sÄ±z modeller: {len(failed_models)} - {failed_models}")
    
    if successful_models:
        print(f"\nğŸ‰ Multi-model veritabanÄ± baÅŸarÄ±yla oluÅŸturuldu!")
        print(f"ğŸš€ Flask uygulamasÄ±nÄ± baÅŸlatabilirsiniz: python app.py")
        return True
    else:
        print(f"\nğŸ’¥ HiÃ§bir model baÅŸarÄ±yla yÃ¼klenemedi!")
        return False

def get_directory_size(path):
    """Dizin boyutunu MB cinsinden hesapla"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            total_size += os.path.getsize(filepath)
    return total_size / (1024 * 1024)  # MB

if __name__ == "__main__":
    main() 